{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9viZZxuFxVSpWfj1XgHUb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalWilk45/MichalWilk45/blob/main/DS_2_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVVkZ62sKhRG",
        "outputId": "e47a2185-360a-4134-d8b6-8fa6e090532e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=4c1dba15accb3e2ac451b3ab4004c14af8c92a119564341a45b37e40c40817d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext(\"local\", \"First App\")\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = sc.parallelize(data)\n"
      ],
      "metadata": {
        "id": "S9q2IghoKm7a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "0S_hCuvgLHd7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"First App\").getOrCreate()\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 28)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LruKQwDbKn_5",
        "outputId": "b1fa828e-98d0-45b0-fc99-6cc313a3259d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 34|\n",
            "|    Bob| 45|\n",
            "|Charlie| 28|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Przykład użycia funkcji okna\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "data = [(\"A\", 1), (\"A\", 2), (\"A\", 3), (\"B\", 4), (\"B\", 5), (\"C\", 6)]\n",
        "columns = [\"key\", \"value\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "window_spec = Window.partitionBy(\"key\").orderBy(\"value\")\n",
        "ranked_df = df.withColumn(\"rank\", F.row_number().over(window_spec))\n",
        "ranked_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxJbdEyqLKeg",
        "outputId": "c649594c-70f0-4b37-f318-130928b24858"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----+\n",
            "|key|value|rank|\n",
            "+---+-----+----+\n",
            "|  A|    1|   1|\n",
            "|  A|    2|   2|\n",
            "|  A|    3|   3|\n",
            "|  B|    4|   1|\n",
            "|  B|    5|   2|\n",
            "|  C|    6|   1|\n",
            "+---+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Przykład użycia funkcji agregujących\n",
        "grouped_df = df.groupBy(\"key\").agg(F.sum(\"value\").alias(\"sum\"), F.mean(\"value\").alias(\"mean\"))\n",
        "grouped_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpUQS6uaLcng",
        "outputId": "524af3c1-da13-44c3-a842-88b3a6eeff05"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+----+\n",
            "|key|sum|mean|\n",
            "+---+---+----+\n",
            "|  B|  9| 4.5|\n",
            "|  C|  6| 6.0|\n",
            "|  A|  6| 2.0|\n",
            "+---+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "uE3S6uuXLpzI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wygeneruj dane testowe\n",
        "data = spark.createDataFrame([\n",
        "  (0, 1.2, 1.3, 1.0),\n",
        "  (1, 2.2, 3.3, 0.0),\n",
        "  (1, 3.2, 4.3, 1.0),\n",
        "  (0, 4.2, 5.3, 0.0),\n",
        "  (0, 5.2, 6.3, 1.0),\n",
        "  (1, 6.2, 7.3, 0.0),\n",
        "  (1, 7.2, 8.3, 1.0),\n",
        "  (0, 8.2, 9.3, 0.0),\n",
        "  (1, 9.2, 1.3, 1.0),\n",
        "  (1, 10.2, 2.3, 0.0)\n",
        "], [\"label\", \"feature1\", \"feature2\", \"feature3\"])"
      ],
      "metadata": {
        "id": "YFJDyleuLrJY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembler=VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"],outputCol=\"features\")"
      ],
      "metadata": {
        "id": "hOwvkcL_LskI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = assembler.transform(data)"
      ],
      "metadata": {
        "id": "8h9CZOnmLvJo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data=data.randomSplit([0.7,0.3],seed=123)"
      ],
      "metadata": {
        "id": "EBZIowj3LwfY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=LogisticRegression(featuresCol=\"features\", labelCol=\"label\")"
      ],
      "metadata": {
        "id": "7Fgm7O85LyH4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline=Pipeline(stages=[lr])\n",
        "\n",
        "model=pipeline.fit(train_data)\n",
        "\n",
        "predictions=model.transform(test_data)\n",
        "\n",
        "evaluator= BinaryClassificationEvaluator(labelCol='label',rawPredictionCol='rawPrediction', metricName='areaUnderROC',)\n",
        "\n",
        "auc=evaluator.evaluate(predictions)\n",
        "\n",
        "predictions.select('label','prediction','probability').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snTWmLEkLzRA",
        "outputId": "e28e236b-8b87-4366-db14-84fe464d8013"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+--------------------+\n",
            "|label|prediction|         probability|\n",
            "+-----+----------+--------------------+\n",
            "|    0|       1.0|[0.30165776275201...|\n",
            "|    1|       0.0|[0.64927965077696...|\n",
            "|    1|       1.0|[0.25944687123224...|\n",
            "+-----+----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"First App\").getOrCreate()"
      ],
      "metadata": {
        "id": "wlN4GHyML0R4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wygeneruj DataFrame z jedną kolumną liczba (liczby całkowite od 1 do 100). Następnie wybierz liczby podzielne przez 3 i pomnóż je przez 2."
      ],
      "metadata": {
        "id": "ADpljUtOOqqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"First App\").getOrCreate()"
      ],
      "metadata": {
        "id": "bQIxjECuOsZc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(i, ) for i in range(1,101)]\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUt2P3M7PJgi",
        "outputId": "5f74fe0f-7a10-4e95-9247-ac1594d3d0b1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1,),\n",
              " (2,),\n",
              " (3,),\n",
              " (4,),\n",
              " (5,),\n",
              " (6,),\n",
              " (7,),\n",
              " (8,),\n",
              " (9,),\n",
              " (10,),\n",
              " (11,),\n",
              " (12,),\n",
              " (13,),\n",
              " (14,),\n",
              " (15,),\n",
              " (16,),\n",
              " (17,),\n",
              " (18,),\n",
              " (19,),\n",
              " (20,),\n",
              " (21,),\n",
              " (22,),\n",
              " (23,),\n",
              " (24,),\n",
              " (25,),\n",
              " (26,),\n",
              " (27,),\n",
              " (28,),\n",
              " (29,),\n",
              " (30,),\n",
              " (31,),\n",
              " (32,),\n",
              " (33,),\n",
              " (34,),\n",
              " (35,),\n",
              " (36,),\n",
              " (37,),\n",
              " (38,),\n",
              " (39,),\n",
              " (40,),\n",
              " (41,),\n",
              " (42,),\n",
              " (43,),\n",
              " (44,),\n",
              " (45,),\n",
              " (46,),\n",
              " (47,),\n",
              " (48,),\n",
              " (49,),\n",
              " (50,),\n",
              " (51,),\n",
              " (52,),\n",
              " (53,),\n",
              " (54,),\n",
              " (55,),\n",
              " (56,),\n",
              " (57,),\n",
              " (58,),\n",
              " (59,),\n",
              " (60,),\n",
              " (61,),\n",
              " (62,),\n",
              " (63,),\n",
              " (64,),\n",
              " (65,),\n",
              " (66,),\n",
              " (67,),\n",
              " (68,),\n",
              " (69,),\n",
              " (70,),\n",
              " (71,),\n",
              " (72,),\n",
              " (73,),\n",
              " (74,),\n",
              " (75,),\n",
              " (76,),\n",
              " (77,),\n",
              " (78,),\n",
              " (79,),\n",
              " (80,),\n",
              " (81,),\n",
              " (82,),\n",
              " (83,),\n",
              " (84,),\n",
              " (85,),\n",
              " (86,),\n",
              " (87,),\n",
              " (88,),\n",
              " (89,),\n",
              " (90,),\n",
              " (91,),\n",
              " (92,),\n",
              " (93,),\n",
              " (94,),\n",
              " (95,),\n",
              " (96,),\n",
              " (97,),\n",
              " (98,),\n",
              " (99,),\n",
              " (100,)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data, ['liczby'])"
      ],
      "metadata": {
        "id": "Qs1nTgGdPpD-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYNgKjIjP9wt",
        "outputId": "6f441c7e-d41a-4b3f-8960-38654238f537"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|liczby|\n",
            "+------+\n",
            "|     1|\n",
            "|     2|\n",
            "|     3|\n",
            "|     4|\n",
            "|     5|\n",
            "|     6|\n",
            "|     7|\n",
            "|     8|\n",
            "|     9|\n",
            "|    10|\n",
            "|    11|\n",
            "|    12|\n",
            "|    13|\n",
            "|    14|\n",
            "|    15|\n",
            "|    16|\n",
            "|    17|\n",
            "|    18|\n",
            "|    19|\n",
            "|    20|\n",
            "+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(col('liczby') %3 == 0).withColumn('multiply', col('liczby')*2).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alj21L06QmVe",
        "outputId": "a67d107e-4b9d-4af4-b7fb-ec78aa82ee95"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+\n",
            "|liczby|multiply|\n",
            "+------+--------+\n",
            "|     3|       6|\n",
            "|     6|      12|\n",
            "|     9|      18|\n",
            "|    12|      24|\n",
            "|    15|      30|\n",
            "|    18|      36|\n",
            "|    21|      42|\n",
            "|    24|      48|\n",
            "|    27|      54|\n",
            "|    30|      60|\n",
            "|    33|      66|\n",
            "|    36|      72|\n",
            "|    39|      78|\n",
            "|    42|      84|\n",
            "|    45|      90|\n",
            "|    48|      96|\n",
            "|    51|     102|\n",
            "|    54|     108|\n",
            "|    57|     114|\n",
            "|    60|     120|\n",
            "+------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "data = [(i,) for i in range(1, 101)]\n",
        "df = spark.createDataFrame(data, ['liczba'])\n",
        "df.filter(col('liczba') % 3 == 0).withColumn('podwojona_liczba', col('liczba') * 2).show()"
      ],
      "metadata": {
        "id": "LJhmaGc1Q9bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ćwiczenie 2:\n",
        "Wygeneruj DataFrame z dwiema kolumnami: imie i nazwisko. Następnie wyświetl wszystkie unikalne nazwiska"
      ],
      "metadata": {
        "id": "VMvxB2YCSAyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('Adam', 'Kowalski'), ('Ewa', 'Nowak'), ('Jan', 'Kowalski'), ('Anna', 'Nowak')]\n",
        "df = spark.createDataFrame(data, ['imie', 'nazwisko'])\n",
        "df.show()\n",
        "df.select(col('nazwisko')).distinct().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_nMNSYmSC7W",
        "outputId": "4986372d-988e-49ff-f2bf-b821669780a7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+\n",
            "|imie|nazwisko|\n",
            "+----+--------+\n",
            "|Adam|Kowalski|\n",
            "| Ewa|   Nowak|\n",
            "| Jan|Kowalski|\n",
            "|Anna|   Nowak|\n",
            "+----+--------+\n",
            "\n",
            "+--------+\n",
            "|nazwisko|\n",
            "+--------+\n",
            "|   Nowak|\n",
            "|Kowalski|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wygeneruj DataFrame z dwiema kolumnami: nazwa i ilosc (liczby całkowite, kolejno: 10, 9, 8, 7, 6, 5, 4, 3, 2, 1). Następnie posortuj DataFrame malejąco po kolumnie ilosc i wyświetl 3 pierwsze wiersze."
      ],
      "metadata": {
        "id": "ZkBxQpzmTgPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('jabłka', 10), ('gruszki', 9), ('śliwki', 8), ('banany', 7), ('winogrona', 6), ('śliwy', 5), ('mandarynki', 4), ('kiwi', 3), ('pomarańcze', 2), ('truskawki', 1)]"
      ],
      "metadata": {
        "id": "Ux4W1CznT89R"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data,['nazwa', 'ilosc'])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZOx3PmvTkTn",
        "outputId": "20697f84-2cbe-4ea9-c2f6-628ca72559ba"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|     nazwa|ilosc|\n",
            "+----------+-----+\n",
            "|    jabłka|   10|\n",
            "|   gruszki|    9|\n",
            "|    śliwki|    8|\n",
            "|    banany|    7|\n",
            "| winogrona|    6|\n",
            "|     śliwy|    5|\n",
            "|mandarynki|    4|\n",
            "|      kiwi|    3|\n",
            "|pomarańcze|    2|\n",
            "| truskawki|    1|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.orderBy(col('ilosc').desc()).show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSogFfb8UCJ0",
        "outputId": "d87e1539-f348-4367-fac8-faa1cda2177c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|  nazwa|ilosc|\n",
            "+-------+-----+\n",
            "| jabłka|   10|\n",
            "|gruszki|    9|\n",
            "| śliwki|    8|\n",
            "+-------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wygeneruj DataFrame z dwiema kolumnami: imie i data_urodzenia (np. \"1990-01-01\"). Następnie oblicz wiek każdej osoby na podstawie daty urodzenia i wyświetl DataFrame wraz z kolumną wiek posortowany malejąco po wieku."
      ],
      "metadata": {
        "id": "rjg6bCK4U8sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import datediff, current_date, to_date\n",
        "\n",
        "data = [('Adam', '1990-01-01'), ('Ewa', '1995-05-15'), ('Jan', '1985-12-30'), ('Anna', '1980-07-12')]\n",
        "df = spark.createDataFrame(data, ['imie', 'data urodzenia'])\n",
        "df.show()\n",
        "df = df.withColumn('data urodzenia', to_date(col('data urodzenia')))\n",
        "df.show()\n",
        "df = df.withColumn('wiek', datediff(current_date(),col(\"data urodzenia\"))/356)\n",
        "df.orderBy(col(\"wiek\").desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeXnLamxU-rc",
        "outputId": "33b94f79-38d9-423e-e6da-1f3947c7e661"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------+\n",
            "|imie|data urodzenia|\n",
            "+----+--------------+\n",
            "|Adam|    1990-01-01|\n",
            "| Ewa|    1995-05-15|\n",
            "| Jan|    1985-12-30|\n",
            "|Anna|    1980-07-12|\n",
            "+----+--------------+\n",
            "\n",
            "+----+--------------+\n",
            "|imie|data urodzenia|\n",
            "+----+--------------+\n",
            "|Adam|    1990-01-01|\n",
            "| Ewa|    1995-05-15|\n",
            "| Jan|    1985-12-30|\n",
            "|Anna|    1980-07-12|\n",
            "+----+--------------+\n",
            "\n",
            "+----+--------------+------------------+\n",
            "|imie|data urodzenia|              wiek|\n",
            "+----+--------------+------------------+\n",
            "|Anna|    1980-07-12| 43.81460674157304|\n",
            "| Jan|    1985-12-30| 38.20505617977528|\n",
            "|Adam|    1990-01-01| 34.09550561797753|\n",
            "| Ewa|    1995-05-15|28.589887640449437|\n",
            "+----+--------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ćwiczenie 5:¶\n",
        "Wygeneruj DataFrame z trzema kolumnami: imie, nazwisko i ocena Następnie stwórz nowy DataFrame z jedną kolumną średnia_ocena dla każdego nazwiska (obliczoną na podstawie wszystkich ocen danej osoby) i wyświetl tylko te wiersze, w których średnia ocena jest większa lub równa 4.5."
      ],
      "metadata": {
        "id": "MS7OpMk_YOqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg, col\n",
        "data = [('Adam', 'Kowalski', 5), ('Ewa', 'Nowak', 4), ('Jan', 'Kowalski', 3), ('Anna', 'Nowak', 5), ('Piotr', 'Kowalski', 5), ('Magda', 'Nowak', 4), ('Karol', 'Kowalski', 4), ('Ania', 'Nowak', 5), ('Kasia', 'Kowalski', 5), ('Michał', 'Kowalski', 5)]\n",
        "df = spark.createDataFrame(data, [\"imie\",\"nazwisko\",\"ocena\"])\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRKhWFWhYPIq",
        "outputId": "1db8e763-aa00-40e1-db6a-8f5f3cdaf829"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-----+\n",
            "|  imie|nazwisko|ocena|\n",
            "+------+--------+-----+\n",
            "|  Adam|Kowalski|    5|\n",
            "|   Ewa|   Nowak|    4|\n",
            "|   Jan|Kowalski|    3|\n",
            "|  Anna|   Nowak|    5|\n",
            "| Piotr|Kowalski|    5|\n",
            "| Magda|   Nowak|    4|\n",
            "| Karol|Kowalski|    4|\n",
            "|  Ania|   Nowak|    5|\n",
            "| Kasia|Kowalski|    5|\n",
            "|Michał|Kowalski|    5|\n",
            "+------+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df = df.groupBy(\"imie\", 'nazwisko').agg(avg(col('ocena')).alias('srednia'))\n",
        "grouped_df.filter(col(\"srednia\") >= 4.5).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "injm8gYVYQeC",
        "outputId": "1ff68e33-74f7-446a-936a-84789c046732"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-------+\n",
            "|  imie|nazwisko|srednia|\n",
            "+------+--------+-------+\n",
            "|  Anna|   Nowak|    5.0|\n",
            "| Piotr|Kowalski|    5.0|\n",
            "|  Adam|Kowalski|    5.0|\n",
            "| Kasia|Kowalski|    5.0|\n",
            "|  Ania|   Nowak|    5.0|\n",
            "|Michał|Kowalski|    5.0|\n",
            "+------+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ćwiczenie 6:\n",
        "Wygeneruj DataFrame z dwiema kolumnami: id (unikalne wartości całkowite od 1 do 10) i tekst (tekst losowy o długości 10 znaków). Następnie dodaj nową kolumnę tekst_zmieniony z tekstem z kolumny tekst, w którym zmienione zostały wszystkie litery na wielkie oraz usunięte zostały spacje. Na koniec wyświetl 5 pierwszych \n",
        "wierszy nowego DataFrame."
      ],
      "metadata": {
        "id": "o2hYoyMjaG1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import upper, regexp_replace\n",
        "\n",
        "data = [(1, 'Lorem ipsuM'), (2, 'DoLor siT AmEt'), (3, 'consectetur Adipiscing'), (4, 'eLit, sed do'), (5, 'tempor Incididunt')]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"id\",\"tekst\"])\n",
        "df = df.withColumn(\"tekst_zmieniony\", regexp_replace(upper(col('tekst')), \" \", \"\"))\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knK1VdqCaJk5",
        "outputId": "f4f94227-37b9-421a-8a76-d70fe5b597c5"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+\n",
            "| id|               tekst|     tekst_zmieniony|\n",
            "+---+--------------------+--------------------+\n",
            "|  1|         Lorem ipsuM|          LOREMIPSUM|\n",
            "|  2|      DoLor siT AmEt|        DOLORSITAMET|\n",
            "|  3|consectetur Adipi...|CONSECTETURADIPIS...|\n",
            "|  4|        eLit, sed do|          ELIT,SEDDO|\n",
            "|  5|   tempor Incididunt|    TEMPORINCIDIDUNT|\n",
            "+---+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_J76OcOBbCb5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}